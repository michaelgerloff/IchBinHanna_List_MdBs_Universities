{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "import googlemaps\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen('https://www.abgeordnetenwatch.de/api/v2/candidacies-mandates?parliament_period=132&range_end=750') as url:\n",
    "    can_man = json.load(url) # used to assign voting districts to member of the Bundestag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install thefuzz\n",
    "#!pip install levenshtein\n",
    "#!pip install -U googlemaps\n",
    "#!pip install geopandas\n",
    "#!pip install folium matplotlib mapclassify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The first part establishes a DataFrame with the name, email adress and voting districts of all members of parliament\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the document \"KÃ¼rschners Volkshandbuch\", which provides names and E-Mail Adresses\n",
    "# The document can be downloaded here: https://www.btg-bestellservice.de/pdf/10037700.pdf; it was converted to a txt file\n",
    "myfile = open(\"Liste_MdBs.txt\", \"rt\")\n",
    "file = myfile.read()\n",
    "# Split the huge string into a list of strings at each new line\n",
    "txt_into_list = file.split(\"\\n\")\n",
    "# extract all E-Mail adresses that end with @bundestag.de\n",
    "result = [x for x in txt_into_list if x.startswith('E-Mail:') and x.endswith('@bundestag.de')]\n",
    "mail_adress = [x.removeprefix('E-Mail: ') for x in result]\n",
    "# reconstruct names by splitting them up at each dot before the @ and combining them again\n",
    "names = [x.removesuffix('@bundestag.de') for x in mail_adress]\n",
    "df_mail = pd.DataFrame(list(zip(names, mail_adress)),\n",
    "                       columns= ['name_conc', 'mail_adress'])\n",
    "df_mail[['Vorname', 'Nachname']] = df_mail.name_conc.str.split('.', expand=True)\n",
    "df_mail[\"Name\"] = df_mail['Vorname'] + \" \" + df_mail['Nachname']\n",
    "# remove the 2 E-Mail adresses that don't correspond to members of parliament\n",
    "df_mail = df_mail.dropna(subset=['Name']).reset_index(drop=True)\n",
    "df_mail.to_csv('Output\\Liste_MdB_Mail.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the list of member of parliament. The list was copied from: https://www.bundestag.de/abgeordnete; You'd need to switch into list view before copying the list.\n",
    "# Importantly the stars that would assign the reasons for why the member of parliament is no longer a member of parliament, were replaced by the character x.\n",
    "df_np = pd.read_csv('Liste_MdB_Namen.csv', header=None)\n",
    "# The party affiliations are stored in even row numbers\n",
    "df_party = df_np[df_np.index % 2 != 0].reset_index(drop=True)\n",
    "df_party.columns = ['Partei', 'junk']\n",
    "#The names of members of parliament are stored in odd row numbers in 2 columns\n",
    "df_name = df_np[df_np.index % 2 == 0].reset_index(drop=True)\n",
    "df_name.columns = ['Nachname', 'Vorname']\n",
    "# combine the names and party affiliations\n",
    "df_name_party = df_name.join(df_party)\n",
    "# remove leading whitespaces from names and party affiliations\n",
    "df_name_party['Nachname'] = df_name_party.Nachname.str.lstrip()\n",
    "df_name_party['Vorname'] = df_name_party.Vorname.str.lstrip()\n",
    "df_name_party['Partei'] = df_name_party.Partei.str.lstrip()\n",
    "# remove everything in parentheses. Parentheses were used when names occured more than once and provided information of the home district of the member of parliament\n",
    "df_name_party['Nachname'] = df_name_party['Nachname'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "# Reconstruct Names by combining names and surnames; export the DataFrame\n",
    "df_name_party['Name'] = df_name_party['Vorname'] + \" \" + df_name_party['Nachname']\n",
    "df_name_party.to_csv('Output\\Liste_MdB_Name_Partei.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to combine the mail DataFrame and the name DataFrame, a common column for joining the DataFrames would be needed.\n",
    "# fuzzy search was applied to connect the reconstructed names from the mail DataFrame with the true names of the name DataFrame\n",
    "# Setting the right algorithm for the fuzzy search is crucial. token_set_ratio provided the best result, even when academic titles were involved.\n",
    "df_mail = pd.read_csv('Output\\Liste_MdB_Mail.csv')\n",
    "df_name_party = pd.read_csv('Output\\Liste_MdB_Name_Partei.csv')\n",
    "df_mail['Name_fuzzy'] = df_mail['Name'].apply(\n",
    "  lambda x: process.extractOne(x, df_name_party['Name'], scorer=fuzz.token_set_ratio)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is optional. The scores of the fuzzy search are calculated and added to the DataFrame df_mail.\n",
    "ratios = []\n",
    "for i in df_mail.index:\n",
    "    ratios.append(fuzz.token_sort_ratio(df_mail['Name'][i], df_mail['Name_fuzzy'][i]))\n",
    "df_mail['Score'] = ratios\n",
    "# Check if there were mismatches. If there were mismatches of the reconstructed and true names, one would expect duplicated entries inm the Name_fuzzy column.\n",
    "df_mail[df_mail.duplicated(['Name_fuzzy'], keep=False)].sort_values(by=['Name_fuzzy', 'Score'])\n",
    "# Export df_mail as csv\n",
    "df_mail.to_csv('Output\\Liste_MdB_Mail_Scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame from the mail DataFrame that only includes the E-mail adresses and the matched names from df_name_party. Rename column Name_fuzzy to Name\n",
    "df_mail_strip = df_mail[['Name_fuzzy', 'mail_adress']].copy()\n",
    "df_mail_strip.columns = ['Name', 'mail_adress']\n",
    "# merge the name_party and mail DataFrames on the column Name\n",
    "data_name_party_adress = pd.merge(df_name_party, df_mail_strip, on='Name', how=\"left\")\n",
    "# remove all previous members of parliament, by removing all entries that contain the character x in the Partei column \n",
    "data_name_party_adress = data_name_party_adress[~data_name_party_adress['Partei'].str.contains('x')].reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for entries that lack E-Mail adresses\n",
    "data_name_party_adress[data_name_party_adress['mail_adress'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab indices of entries without E-mail adresses and add the missing adresses by hand\n",
    "na_ind = data_name_party_adress[data_name_party_adress['mail_adress'].isna()].index\n",
    "data_name_party_adress.loc[na_ind, 'mail_adress'] = ['melanie.bernstein@bundestag.de', 'alexander.foehr@bundestag.de', 'dirk-ulrich.mende@bundestag.de', 'Rainer.Rothfuss@bundestag.de', 'ana-maria-trasnea@bundestag.de.', 'emily.vontz@bundestag.de']\n",
    "# Export the DataFrame\n",
    "data_name_party_adress.to_csv('Output\\Liste_MdB_Name_Partei_Adresse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the json file from the API of abgeordnetenwatch.de to assign voting districts to the members of the Bundestag\n",
    "# extract the name of a politician\n",
    "can_man.get('data')[250].get('politician').get('label')\n",
    "# extract the voting district\n",
    "can_man.get('data')[250].get('electoral_data').get('constituency').get('label')\n",
    "# create 2 lists with names and coresponding voting districts\n",
    "length = len(can_man.get('data'))\n",
    "name = []\n",
    "wahlkreis = []\n",
    "for i in range(length):\n",
    "    name.append(can_man.get('data')[i].get('politician').get('label'))\n",
    "    if can_man.get('data')[i].get('electoral_data').get('constituency') == None:\n",
    "        wahlkreis.append('None')\n",
    "    else:\n",
    "         wahlkreis.append(can_man.get('data')[i].get('electoral_data').get('constituency').get('label'))\n",
    "\n",
    "# combine the 2 lists to one DataFrame\n",
    "mdb_wkr = pd.DataFrame(list(zip(name, wahlkreis)), columns=['Name', 'Wahlkreis'])\n",
    "# remove everything in parentheses (Bundestag 2021-2025)\n",
    "mdb_wkr['Wahlkreis'] = mdb_wkr['Wahlkreis'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "# Split the number of the voting district and its name\n",
    "mdb_wkr[['Wahlkreis', 'Wahlkreisname']] = mdb_wkr.Wahlkreis.str.split(\" - \", 1, expand=True)\n",
    "# create a common column with df_name_party and data_name_party_adress\n",
    "mdb_wkr['Name_fuzzy'] = mdb_wkr['Name'].apply(\n",
    "  lambda x: process.extractOne(x, df_name_party['Name'], scorer=fuzz.token_set_ratio)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is optional. The scores of the fuzzy search are calculated and added to the DataFrame mdb_wkr.\n",
    "ratios = []\n",
    "for i in mdb_wkr.index:\n",
    "    ratios.append(fuzz.token_sort_ratio(mdb_wkr['Name'][i], mdb_wkr['Name_fuzzy'][i]))\n",
    "mdb_wkr['Score'] = ratios\n",
    "# Check if there were mismatches. If there were mismatches of the reconstructed and true names, one would expect duplicated entries inm the Name_fuzzy column.\n",
    "mdb_wkr[mdb_wkr.duplicated(['Name_fuzzy'], keep=False)].sort_values(by=['Name_fuzzy', 'Score'])\n",
    "mdb_wkr.to_csv('Output\\Liste_MdB_Wahlkreis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame from the mail DataFrame that only includes the E-mail adresses and the matched names from df_name_party. Rename column Name_fuzzy to Name\n",
    "mdb_wkr_strip = mdb_wkr[['Name_fuzzy', 'Wahlkreis', 'Wahlkreisname']].copy()\n",
    "mdb_wkr_strip.columns = ['Name', 'Wahlkreis', 'Wahlkreisname']\n",
    "# merge the name_party and mail DataFrames on the column Name\n",
    "data_name_party_adress_wkr = pd.merge(data_name_party_adress, mdb_wkr_strip, on='Name', how=\"left\")\n",
    "# convert the column to numeric and replace 'None' with nan\n",
    "data_name_party_adress_wkr['Wahlkreis'] = pd.to_numeric(data_name_party_adress_wkr['Wahlkreis'], errors='coerce')\n",
    "# Export the DataFrame\n",
    "data_name_party_adress_wkr.to_csv('Output\\Liste_MdB_Name_Partei_Adresse_Wahlkreis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The second part establishes a DataFrame with all universities and their corresponding voting districts\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this only if you have a valid Google Maps API key. If you don't have one skip this cell and proceed with the next one. You'll find a hs_liste_koordinaten.csv file with all coordinates in the output folder. \n",
    "\n",
    "api = open(\"Google maps api.txt\", \"rt\")\n",
    "api_str = api.read()\n",
    "# Add your individual Google Maps API key\n",
    "gmaps = googlemaps.Client(key=api_str)\n",
    "\n",
    "# import DataFrame with a list of all universities in Germany. Concatenate the adressess\n",
    "df_hochschulen = pd.read_csv('hs_liste.txt', sep='\\t', dtype=str)\n",
    "adress = df_hochschulen['StraÃe'] + ', ' + df_hochschulen['Postleitzahl (Hausanschrift)'] + ', ' + df_hochschulen['Ort (Hausanschrift)']\n",
    "df_hochschulen['Adresse'] = adress\n",
    "\n",
    "# find the coordinates for all university adresses. Store the values in separate lists\n",
    "lattitude = []\n",
    "longitude = []\n",
    "# Do this only once since it grabs the information via the googlemaps API which isn't technically free\n",
    "for i in df_hochschulen.index:\n",
    "    x = gmaps.geocode(df_hochschulen['Adresse'][i])\n",
    "    lattitude.append(x[0].get('geometry').get('location')['lat'])\n",
    "    longitude.append(x[0].get('geometry').get('location')['lng'])\n",
    "# add the coordinates to the original DataFrame and save it.\n",
    "df_hochschulen['Latitude'] = lattitude\n",
    "df_hochschulen['Longitude'] = longitude\n",
    "df_hochschulen.to_csv('Output\\hs_liste_koordinaten.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the shp files of all voting districts in Germany and change their reference system to degrees\n",
    "# download the zip file from: https://www.bundeswahlleiterin.de/dam/jcr/fc2f7796-cc84-4f89-b475-4439fcc8fa07/btw21_geometrie_wahlkreise_shp.zip and extract to \\Wahlkreise\n",
    "# You can find all kind of data regarding voting districts here: https://www.bundeswahlleiterin.de/bundestagswahlen/2021/wahlkreiseinteilung/downloads.html\n",
    "Wahlkreise = gpd.read_file('Wahlkreise\\Geometrie_Wahlkreise_20DBT.shp')\n",
    "Wahlkreise_Grad = Wahlkreise.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df_hochschulen DataFrame a geopanda GeoDataFrame by specifying the columns with longitude and latitude and specifying the degree reference system\n",
    "df_hochschulen = pd.read_csv('Output\\hs_liste_koordinaten.csv', sep='\\t') # load hs_liste_koordinaten.csv in case you don't have a Google Maps API\n",
    "df_hochschulen_shp = gpd.GeoDataFrame(\n",
    "    df_hochschulen, geometry=gpd.points_from_xy(df_hochschulen.Longitude, df_hochschulen.Latitude), crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: plot all universities onto the borders of all voting districts\n",
    "ax = Wahlkreise_Grad.plot(color=\"white\", edgecolor=\"black\")\n",
    "df_hochschulen_shp.plot(ax = ax, color = 'red', markersize = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the voting districts to the universities\n",
    "df_hochschulen_wkr = gpd.tools.sjoin(df_hochschulen_shp, Wahlkreise_Grad, predicate = 'within', how = 'left')\n",
    "# export the DataFrame\n",
    "df_hochschulen_wkr.to_csv('Output\\hs_liste_koordinaten_Wahlkreise.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The third part combines universities and member of parliament\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that combines the names, Email-Adresses and the names of universities\n",
    "list_hs_kurz = []\n",
    "list_hs_lang = []\n",
    "for i in data_name_party_adress_wkr.index:\n",
    "    list_hs_kurz.append(df_hochschulen_wkr[(df_hochschulen_wkr.WKR_NR == data_name_party_adress_wkr['Wahlkreis'][i])]['Hochschulkurzname'].tolist())\n",
    "    list_hs_lang.append(df_hochschulen_wkr[(df_hochschulen_wkr.WKR_NR == data_name_party_adress_wkr['Wahlkreis'][i])]['Hochschulname'].tolist())\n",
    "\n",
    "data_name_party_adress_wkr_hs = data_name_party_adress_wkr\n",
    "data_name_party_adress_wkr_hs['HS_Kurzname'] = list_hs_kurz\n",
    "data_name_party_adress_wkr_hs['HS_Name'] = list_hs_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all MdBs that don't represent a district with at least 1 university\n",
    "data_name_party_adress_wkr_hs = data_name_party_adress_wkr_hs[data_name_party_adress_wkr_hs['HS_Name'].map(lambda d: len(d)) > 0]\n",
    "# drop unimportant columns\n",
    "data_name_party_adress_wkr_hs.drop(['Unnamed: 0', 'Nachname', 'Vorname', 'junk', 'Wahlkreisname'], axis=1, inplace=True)\n",
    "data_name_party_adress_wkr_hs.to_csv('Output\\Liste_MdB_Name_Partei_Adresse_Wahlkreis_Hochschule.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that combines, universities, Names and Email-Adresses of the MdBs\n",
    "\n",
    "#first create empty lists for the names and adresses for the invdividual parties\n",
    "SPD_Name = []\n",
    "SPD_Adr = []\n",
    "CDU_Name = []\n",
    "CDU_Adr = []\n",
    "Green_Name = []\n",
    "Green_Adr = []\n",
    "FDP_Name = []\n",
    "FDP_Adr = []\n",
    "Linke_Name = []\n",
    "Linke_Adr = []\n",
    "AFD_Name = []\n",
    "AFD_Adr = []\n",
    "\n",
    "# iterate through each row of df_hochschulen_wkr and look in data_name_party_adress_wkr whether there is an MdB of the respective party. If not append 'none' to the lists, if yes append the Name and Email Adresses to the respective lists\n",
    "for i in df_hochschulen_wkr.index:\n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'SPD')]['Name']) == 1:\n",
    "        SPD_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'SPD')]['Name'].reset_index(drop=True)[0])\n",
    "        SPD_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'SPD')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        SPD_Name.append('None')\n",
    "        SPD_Adr.append('None')\n",
    "    \n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'CDU/CSU')]['Name']) == 1:\n",
    "        CDU_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'CDU/CSU')]['Name'].reset_index(drop=True)[0])\n",
    "        CDU_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'CDU/CSU')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        CDU_Name.append('None')\n",
    "        CDU_Adr.append('None')\n",
    "\n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'BÃ¼ndnis 90/Die GrÃ¼nen')]['Name']) == 1:\n",
    "        Green_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'BÃ¼ndnis 90/Die GrÃ¼nen')]['Name'].reset_index(drop=True)[0])\n",
    "        Green_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'BÃ¼ndnis 90/Die GrÃ¼nen')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        Green_Name.append('None')\n",
    "        Green_Adr.append('None')\n",
    "\n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'FDP')]['Name']) == 1:\n",
    "        FDP_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'FDP')]['Name'].reset_index(drop=True)[0])\n",
    "        FDP_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'FDP')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        FDP_Name.append('None')\n",
    "        FDP_Adr.append('None')\n",
    "\n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'Die Linke')]['Name']) == 1:\n",
    "        Linke_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'Die Linke')]['Name'].reset_index(drop=True)[0])\n",
    "        Linke_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'Die Linke')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        Linke_Name.append('None')\n",
    "        Linke_Adr.append('None')\n",
    "\n",
    "    if len(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'AfD')]['Name']) == 1:\n",
    "        AFD_Name.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'AfD')]['Name'].reset_index(drop=True)[0])\n",
    "        AFD_Adr.append(data_name_party_adress_wkr[(data_name_party_adress_wkr.Wahlkreis == df_hochschulen_wkr.WKR_NR[i]) & (data_name_party_adress_wkr.Partei == 'AfD')]['mail_adress'].reset_index(drop=True)[0])\n",
    "    else:\n",
    "        AFD_Name.append('None')\n",
    "        AFD_Adr.append('None')\n",
    "\n",
    "#Create a temporary Dataframe with the filled lists and join it with df_hochschulen_wkr\n",
    "temp = pd.DataFrame(list(zip(SPD_Name, SPD_Adr, CDU_Name, CDU_Adr, Green_Name, Green_Adr, FDP_Name, FDP_Adr, Linke_Name, Linke_Adr, AFD_Name, AFD_Adr)), \n",
    "                    columns=['SPD_Name', 'SPD_Adr', 'CDU_Name', 'CDU_Adr', 'Green_Name', 'Green_Adr', 'FDP_Name', 'FDP_Adr', 'Linke_Name', 'Linke_Adr', 'AFD_Name', 'AFD_Adr']\n",
    ")\n",
    "\n",
    "df_hochschulen_wkr_mdb = pd.concat([df_hochschulen_wkr, temp], axis = 1, join = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with only important columns\n",
    "df_hochschulen_wkr_mdb_strip = df_hochschulen_wkr_mdb[['Hochschulkurzname', 'Hochschulname', 'WKR_NR', 'SPD_Name', 'SPD_Adr', 'CDU_Name', 'CDU_Adr', 'Green_Name', 'Green_Adr', 'FDP_Name', 'FDP_Adr', 'Linke_Name', 'Linke_Adr', 'AFD_Name', 'AFD_Adr', 'Hochschultyp', 'TrÃ¤gerschaft', 'Bundesland']].copy()\n",
    "df_hochschulen_wkr_mdb_strip.to_csv('Output\\hs_liste_Wahlkreise_mdb.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
